---
title: "ex2"
author: "shreyakolte"
format: html
editor: visual
---

## DATA PROCESSING USING R:

## Installing required packages:

```{r}
#if (!require("pacman"))
```

```{r}
if (!require("pacman")) 
  {
  install.packages("pacman")
  library(pacman)
}


```

```{r}
p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr) 
```

### Loading data:

```{r}
data <- read_csv(here("data", "x.csv"))

data |> glimpse()
```

### Tab separated values:

```{r}
data <- read_delim(here("data", "x.tsv"))

data |> glimpse()
```

## **Importing data from MySQL database:**

Connecting to a database in MySQL database management system.

```{r}
drv <- dbDriver("MySQL") #obtain the driver for MySQL, drivers available for other DBMS
```

### **Using dplyr instead:**

```{r}
if (!require("dbplyr")) 
install.packages("dbplyr") #install but don’t run library() on this dbplyr.
```

### **Obtain a connection:**

```{r}
#con <- src_mysql("etcsite_charaparser", user = "shreyakolte", password = #"shreyakolte", host = "localhost")
```

Get an entire table as tb1

```{r}
#allwords <- tbl(con, "1_allwords")
#allwords
```

[**encountering an error while obtaining a database connection in my system.**]{.underline}

# **Data Cleaning**

# **Wide vs. long format:**

Read data in wide format:

```{r}
wide <- read_delim(here("data", "wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

The wide format uses the values (math, english) of variable subjects as variables.

The long format should have Name, Subject, and Grade as variables (i.e., columns).

```{r}
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

## **Long to wide, use spread():**

```{r}
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

## **Split a column into multiple columns:**

Splitting Degree_Year to Degree and Year

```{r}
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

## **Handling date/time and time zones:**

```{r}
install.packages("lubridate")
library(lubridate)
```

**Convert dates of variance formats into one format:**

```{r}
mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day format
clean.dates
```

**Extract day, week, month, year info from dates:**

```{r}
data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

**Time zone:**

```{r}
date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

**Convert to Phoenix, AZ time:**

```{r}
with_tz(date.time, tz="America/Phoenix")
```

**Change the timezone for a time:**

```{r}
force_tz(date.time, "Turkey")
```

**Check available time zones:**

```{r}
OlsonNames()
```

## **String Processing:**

Common needs: stringr package

Advanced needs: stringi package

```{r}
library(dplyr)
library(stringr)
library(readr)
```

**Fetching data from a URL, form the URL using string functions:**

```{r}
uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

**str_c: string concatenation:**

```{r}
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

**Read the data file:**

```{r}
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
dim(data)
```

**Read the name file line by line, put the lines in a vector:**

```{r}
lines <- read_lines(url(namesF))

lines |> head()
```

Examining the content of lines and see the column names starting on line 67 to line 135. Then, we get column name lines and clean up to get column names:

```{r}
names <- lines[67:135]
names
```

Observation: a name line consists two parts, name: valid values. The part before : is the name.

```{r}
names <- str_split_fixed(names, ":", 2) #split on regular expression pattern ":", this function returns a matrix
names
```

**Take the first column, which contains names:**

```{r}
names <- names[,1]
names
```

**Now cleaning up the names: trim spaces, remove ( ):**

```{r}
names <-str_trim(names) |> str_replace_all("\\(|\\)", "") # we use a pipe, and another reg exp "\\(|\\)", \\ is the escape.
names
```

**Finally, putting the columns to the data:**

```{r}
colnames(data)[1:69] <- names
data
```

**Rename the last two columns:**

```{r}
colnames(data)[70:71] <- c("id", "class")
data
```

## **Dealing with unknown values:**

Removing observations or columns with many NAs:

```{r}
library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows
```

```{r}
data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

Maximum missing values in a row is 7, out of 69 dimensions, so they are not too bad.

Examine columns: how many NAs in each variable/column?

```{r}
install.packages("tidyr")
library(tidyr)
data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

bser variable has **196 NAs**. If this variable is considered not useful, given some domain knowledge, we can remove it from the data. From View, I can see bser is the 8th column:

```{r}
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

```{r}
data <- data %>%
  select(-matches("bser"))
```

### **Mistaken characters:**

Because R decides the data type based on what is given, sometimes, R's decision may not be what you meant. In the example below, because of a missing value `?`, R makes all other values in a vector 'character'. Parse_integer can be used to fix this problem. 

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

### **Filling unknowns with most frequent values:**

```{r}
if (!require("DMwR2")) 
install.packages("DMwR2")
library(DMwR2)
data(algae, package = "DMwR2")
algae[48,]
```

mxph is unknown. Shall we fill in with mean, median or something else?

```{r}
# plot a QQ plot of mxPH
if (!require("car")) 
install.packages("car")
library(car)
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

The straight line fits the data pretty well so `mxPH`is normal, use mean to fill the unknown.

```{r}
algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

What about attribute Chla?

```{r}
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
median(algae$Chla, na.rm = TRUE)
```

```{r}
mean(algae$Chla, na.rm = TRUE)
```

Obviously, the mean is not a representative value for `Chla`. For this we will use median to fill all missing values in this attribute, instead of doing it one value at a time:

```{r}
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

### **Filling unknowns using linear regression**

This method is used when two variables are highly correlated. One value of variable A can be used to predict the value for variable B using the linear regression model.

```{r}
library(palmerpenguins)
library(tidyverse)
```

```{r}
if (!require("corrr")) 
install.packages("corrr")
library(corrr)
```

```{r}
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

Next, we find the linear model between `PO4` and `oPO4`:

```{r}
algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)#this is a method provided that selects the observations with 20% or move values as NAs. 

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

```{r}
m |> 
  summary()
```

If a good model, coefficients should all be significant (reject Ho coefficience is 0), Adjusted R-squared close to 1 (0.8 is very good).

F-statistics p-value should be less than the significant level (typically 0.05).

While R-squared provides an estimate of the strength of the relationship between your model and the response variable, it does not provide a formal hypothesis test for this relationship.

The F-test of overall significance determines whether this relationship is statistically significant.

```{r}
algae$PO4
```

```{r}
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))
```

```{r}
res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

Create a simple function `fillPO4`:

```{r}
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
#if x is not NA, return 42.897+1.293*x 
```

```{r}
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

Apply calls fillPO4 function repeatedly, each time using one value in `algae[is.na(algae$PO4), "oPO4"]` as an argument.

### **Filling unknowns by exploring similarities among cases:**

```{r}
data(algae, package="DMwR2")
algae <- algae[-manyNAs(algae), ] 
```

DM2R2 provides a method call knnImputation. This method use the Euclidean distance to find the ten most similar cases of any water sample with some unknown value in a variable, and then use their values to fill in the unknown.

```{r}
algae <- knnImputation(algae, k = 10) #use the weighted average of k most similar samples


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of k most similar samples
```

To see what is in `knnImputation()` (You are not required to read and understand the code):

```{r}
getAnywhere(knnImputation())
```

# **Scaling and normalization:**

Normalizing the values in penguins dataset:

```{r}
library(dplyr)
library(palmerpenguins)
```

```{r}
data(penguins)
```

```{r}
# select only numeric columns
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# normalize numeric columns
penguins_norm <- scale(penguins_numeric)

# convert back to data frame and add species column
peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)

# because scale() takes numeric matrix as input, we first remove Species column, then use cbind() to add the column back after normalization.
```

```{r}
summary(penguins)
```

```{r}
summary(peng.norm)
```

scale() can also take an argument for center and an argument of scale to normalize data in some other ways

```{r}
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min
```

```{r}
# min-max normalization
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

## **Discretizing variables (binning)**

The process of transferring continuous functions, models, variables, and equations into discrete counterparts

Use dlookr's binning (type="equal") for equal-length cuts (bins)

Use Hmisc's cut2() for equal-depth cuts

Boston Housing data as an example:

```{r}
data(Boston, package="MASS")
summary(Boston$age)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") #create 5 bins and add new column newAge to Boston
summary(Boston$newAge)
```

```{r}
Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal") #add labels

summary(Boston$newAge)
```

### **Equal-depth:**

```{r}
if (!require("Hmisc")) 
install.packages("Hmisc")
library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins and add new column newAge to #Boston

table(Boston$newAge)
```

### **Assign labels:**

```{r}
Boston$newAge <- factor(cut(Boston$age, breaks = quantile(Boston$age, probs = seq(0, 1, 0.2))), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

Plot an equal-width histogram of width 10:

```{r}
hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 – 101.
```

or use ggplot2:

```{r}
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

## **Decimal scaling:**

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
(nDigits = nchar(max(abs(data)))) #nchar counts the number of characters
```

```{r}
(decimalScale = data / (10^nDigits))
```

### **Smoothing by bin mean:**

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate data into bins of depth 3
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

**Find the average of each bin:**

```{r}
(bin_means = apply(bins, 1, FUN = mean))
```

**Replace values with their bin mean:**

```{r}
for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

# **Variable correlations and dimensionality reduction**

## **Chi-squared test**

H0: (Prisoner's race)(Victim's race) are independent.

data (contingency table):

```{r}
racetable = rbind(c(151,9), c(63,103))
test1 = chisq.test(racetable, correct=F)
test1
```

p-value is less than 0.05: chance to get X-squared value of 115.01 assuming H0 is true is very slim (close to 0), so reject H0.

## **Loglinear model**

Extending chi-squared to more than 2 categorical variables.

Loglinear models model cell counts in contingency tables.

```{r}
seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
seniors
```

Next, do loglinear modeling using the glm function (generalized linear models).

We need to convert the array to a table then to a data frame.

```{r}
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

Next, we model Freq (this is the count in the contingency table) as a function of the three variables using the glm function. Set `family = poisson` because we are assuming independent counts. 

Poisson distribution: discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event.

```{r}
mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

Now lets' remove age and re-generate a model with the remaining three variables.

```{r}
mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

We see the model fits well, and most effects are significant now.

The insignificant one is the 3-way interaction among the three factors

For data reduction, we are done -- we removed `age` variable. Because all cigarette, marijuana, and alcohol effects are significant, we can't remove any of these from the data set, even though the 3-way interaction is not significant.

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

Now compare the fitted and observed values and see how well they match up:

```{r}
cbind(mod.3$data, fitted(mod.3))
```

## **Correlations:**

```{r}
library(tidyr) # data manipulation
penguins_numeric |> 
  drop_na() |>
  correlate()
```

## **Principal components analysis (PCA):**

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
head(pca$scores) # pca result is a list, and the component scores are elements in the list
```

If we are happy with capturing 75% of the original variance of the cases, we can reduce the original data to the first three components.

Component scores are computed based on the loading, for example:

```{r}
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
if (!require("wavelets")) 
install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3) #with 8-element vector, 3 level is the max.
wt
```

```{r}
idwt(wt)
```

Obtain transform results as shown in class, use a different filter:

```{r}
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

Reconstruct the original:

```{r}
idwt(xt)
```

# **Sampling:**

```{r}
set.seed(1)
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

## **Simple random sampling, without replacement:**

```{r}
sample(age, 5)
```

## **Simple random sampling, with replacement:**

```{r}
sample(age, 5, replace = TRUE)
```

## **Stratified sampling:**

```{r}
library(dplyr)
set.seed(1) #make results the same each run
summary(algae)
```

```{r}
sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

## **Cluster sampling:**

```{r}
if (!require("sampling")) 
install.packages("sampling")
library(sampling)
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)
ageframe$condition <- s$cluster # add cluster label as condition
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

# **Handling Text Datasets:**

```{r}
pacman::p_load(tm,
               SnowballC)#tm uses SnowballC for stemming
# read corpus
# Emails.csv,  holding some of Hillary's emails
data <- read.csv(here::here("data", "Emails.csv"), stringsAsFactors = FALSE)

docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

## **Inspect a document:**

```{r}
docs[[20]]
```

## **Preprocessing text:**

```{r}
docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

**Convert text to a matrix using `TF*IDF scores` (see `TF*IDF` scores in Han's text):**

```{r}
DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
DTData
```

```{r}
inspect(DTData[1:2, 1:5])
```

**Create term-document matrix (also called inverted index, see Han's text in a later chapter):**

```{r}
TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

## **Explore the dataset:**

```{r}
findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)
```

**Find correlations among terms:**

```{r}
findAssocs(TDData, terms = "bill", corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)
```

```{r}
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

## **Create a word cloud:**

```{r}
if (!require("wordcloud")) 
install.packages("wordcloud")
if (!require("RColorBrewer")) 
install.packages("RColorBrewer")
library(wordcloud)
```

Loading required package: RColorBrewer

```{r}
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

```{r}
png(file = "wordCloud.png", width = 1000, height = 700, bg= "grey30")

wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

`dev.off()` closed the `.png` file, now the current display is the default display in RStudio. Use `dev.list()` to find the graphics devices that are active, repeatedly use `dev.off()` to close devices that not needed. R Studio is the default display. When all other devices are closed, the default display is used.

```{r}
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

Sometimes you need to one-hot encoding a section of a dataframe. You can do it by using onehot package.

```{r}
if (!require("onehot")) 
install.packages("onehot")
library(onehot)
d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
d$language <- as.factor(d$language) #convert the column to be encoded to Factor
encoded <- onehot(d)
new_d <- predict(encoded, d)
new_d
```

One hot encoding for data frame with multi-value cells (`language = "javascript, python"`)

```{r}
if (!require("qdapTools")) 
install.packages("qdapTools")
library(qdapTools)
d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```

# **\[ADVANCED\]:**

Exercises on your data set:

1.  What attributes are there in your data set? Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations

```{r}
# Load the 'ggplot2' package and access the 'diamonds' dataset
library(ggplot2)
data(mtcars)

# Use the names() function to view the attributes
names(mtcars)
 
```

```{r}
# Alternatively, you can use the colnames() function
colnames(mtcars)

```

```{r}
summary(mtcars)
```

2.  Do you have highly correlated attributes? How did you find out about the correlations or lack of correlations?

```{r}
# Load the corrplot package (install it if not already installed)
library(corrplot)

# Calculate the correlation matrix for the mtcars dataset
correlation_matrix <- cor(mtcars)

# Plot the correlation matrix using corrplot
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust")

```

3.  Do you have numerical attributes that you might want to bin? Try at least two methods and compare the differences.

    **solution:**

    **1. Equal Frequency Binning:**

    Equal frequency binning, also known as quantile binning, divides the data into bins such that each bin contains approximately the same number of data points.

    We use the **`quantile()`** function to calculate quartiles and then use those quartiles as breaks for binning the "mpg" attribute. The resulting bins will have approximately the same number of data points in each.

```{r}
# Perform equal frequency binning on the "mpg" attribute
mtcars$mpg_equal_freq <- cut(mtcars$mpg, breaks = quantile(mtcars$mpg, probs = seq(0, 1, 0.25), na.rm = TRUE), labels = FALSE)

# View the resulting bins
table(mtcars$mpg_equal_freq)
```

2.  **Equal Depth Binning:**

Equal depth binning, also known as equal width binning, divides the data into bins such that each bin has a specified range of values. In R, you can use the **`cut()`** function to perform equal depth binning.

In this code, we use the **`seq()`** function to create equally spaced breaks based on the minimum and maximum values of the "mpg" attribute. The resulting bins will have equal width in terms of the "mpg" values.

```{r}
# Perform equal depth binning on the "mpg" attribute
mtcars$mpg_equal_depth <- cut(mtcars$mpg, breaks = seq(min(mtcars$mpg), max(mtcars$mpg), length.out = 5), labels = FALSE)

# View the resulting bins
table(mtcars$mpg_equal_depth)

```

4.  If you have categorical attributes, use the concept hierarchy generation heuristics (based on attribute value counts) suggested in the textbook to produce some concept hierarchies. How well does this approach work for your attributes?

```{r}
data(mtcars)
table(mtcars$cyl) #This will show you the frequency of each value in the "cyl" attribute.

mtcars$cyl_category <- ifelse(mtcars$cyl %in% c(4, 6), "Low Cylinders", "High Cylinders") #to create a new categorical attribute 

table(mtcars$cyl_category) #the frequency of each category in the "cyl_category" attribute.

```

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--END\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
